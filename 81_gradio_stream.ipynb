{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. Gradio_Stream\n",
    "\n",
    "## Overview  \n",
    "This exercise demonstrates how to build a Retrieval-Augmented Generation (RAG) system using Gradio and how to generate and stream responses in real-time using its streaming features. Through this exercise, you will learn to handle real-time interactions with users via a web-based interface. This process helps manage the overall conversation flow, thereby providing more detailed and meaningful responses.\n",
    " \n",
    "## Purpose of the Exercise\n",
    "The purpose of this exercise is to implement real-time response generation and streaming capabilities using Gradio to develop a live interactive chatbot interface. By the end of this tutorial, users will be able to create a dynamic chat system that streams responses as they are generated, enhancing user engagement and interaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU gradio python-dotenv langchain-upstage python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title set API key\n",
    "import os\n",
    "import getpass\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython import get_ipython\n",
    "\n",
    "if \"google.colab\" in str(get_ipython()):\n",
    "    # Running in Google Colab. Please set the UPSTAGE_API_KEY in the Colab Secrets\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = userdata.get(\"UPSTAGE_API_KEY\")\n",
    "else:\n",
    "    # Running locally. Please set the UPSTAGE_API_KEY in the .env file\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "if \"UPSTAGE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"UPSTAGE_API_KEY\"] = getpass.getpass(\"Enter your Upstage API key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain_upstage import UpstageDocumentParseLoader, UpstageGroundednessCheck, ChatUpstage, UpstageEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "llm = ChatUpstage(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More general chat\n",
    "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{message}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_loader = UpstageDocumentParseLoader(\"laws.pdf\", output_format='html', coordinates=False)\n",
    "docs = doc_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"<h1 id='0' style='font-size:18px'>ê²½ê¸°ê·œì¹™</h1><br><h1 id='1' \"\n",
      " \"style='font-size:16px'>Laws of the Game</h\")\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    pprint(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits: 234\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Splits:\", len(splits)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic chunking split\n",
    "from langchain.docstore.document import Document\n",
    "def semantic_chunker(docs, min_chunk_size=100, chunk_overlap=10, max_chunk_size=1000, merge_threshold=0.7, embeddings=UpstageEmbeddings(model=\"solar-embedding-1-large\")):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=min_chunk_size, chunk_overlap=chunk_overlap)\n",
    "    init_splits = text_splitter.split_documents(docs)\n",
    "    splits = []\n",
    "\n",
    "    base_split_text = None\n",
    "    base_split_emb = None\n",
    "    for split in init_splits:\n",
    "        if base_split_text is None:\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = embeddings.embed_documents([base_split_text])[0]\n",
    "            continue\n",
    "\n",
    "        split_emb = embeddings.embed_documents([split.page_content])[0]\n",
    "        distance = cosine_similarity(X=[base_split_emb], Y=[split_emb])\n",
    "        if (distance[0][0] < merge_threshold or len(base_split_text) + len(split.page_content) > max_chunk_size):\n",
    "            splits.append(Document(page_content=base_split_text))\n",
    "            base_split_text = split.page_content\n",
    "            base_split_emb = split_emb\n",
    "        else:\n",
    "            base_split_text += split.page_content\n",
    "\n",
    "    if base_split_text:\n",
    "        splits.append(Document(page_content=base_split_text))\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hfembeddings = HuggingFaceEmbeddings(model_name=\"klue/roberta-small\")\n",
    "u_embeddings = UpstageEmbeddings(model=\"solar-embedding-1-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticChunker Splits: 2299\n"
     ]
    }
   ],
   "source": [
    "semantic_splits = semantic_chunker(docs, merge_threshold=0.8, embeddings=u_embeddings)\n",
    "\n",
    "print(\"SemanticChunker Splits:\", len(semantic_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=semantic_splits, embedding=UpstageEmbeddings(model=\"solar-embedding-1-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_pretrained(\"upstage/solar-1-mini-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "chat_with_history_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", \n",
    "            \"\"\"\n",
    "            ë„ˆëŠ” ì¶•êµ¬ ìš©ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ì±—ë´‡ì´ì•¼.\n",
    "            ì œê³µëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ê³ , ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ì— ê¸°ë°˜í•´ì„œ ë‹µë³€í•´ì¤˜.\n",
    "            ë‹µë³€ì„ ëª¨ë¥´ë©´ ê·¸ëƒ¥ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´ì¤˜.\n",
    "            ---\n",
    "            CONTEXT:\n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ), \n",
    "        MessagesPlaceholder(variable_name='history'), \n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_with_history_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(question, retriever):\n",
    "    # retrieverë¥¼ ì‚¬ìš©í•´ ê´€ë ¨ëœ ë¬¸ì„œë§Œ ê°€ì ¸ì˜´\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # ê´€ë ¨ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_2(message, history):\n",
    "    print(message)\n",
    "    history_langchain_format = []\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    generator = chain.stream({\"message\": message, \"history\": history_langchain_format})\n",
    "\n",
    "    assistant = \"\"\n",
    "    for gen in generator:\n",
    "        assistant += gen\n",
    "        yield assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    print(question)\n",
    "    history_langchain_format = []\n",
    "\n",
    "    #####\n",
    "    # ê¸°ì¡´ ì§ˆë¬¸ ë° ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "    relevant_context = get_relevant_context(question, retriever)\n",
    "    # ëª¨ë¸ í˜¸ì¶œ\n",
    "    ai = chain.invoke({\n",
    "        \"history\": history_langchain_format, \n",
    "        \"context\": relevant_context, \n",
    "        \"input\": question\n",
    "    })\n",
    "    #####\n",
    "\n",
    "    # for human, ai in history:\n",
    "    #     history_langchain_format.append(HumanMessage(content=human))\n",
    "    #     history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    #generator = chain.stream({\"message\": question, \"history\": history_langchain_format})\n",
    "\n",
    "    # assistant = \"\"\n",
    "    # for gen in generator:\n",
    "    #     assistant += gen\n",
    "    #     yield assistant\n",
    "\n",
    "    return ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from __future__ import annotations\n",
    "from typing import Iterable\n",
    "from gradio.themes.base import Base\n",
    "from gradio.themes.utils import colors, fonts, sizes\n",
    "class Football(Base):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        primary_hue: colors.Color | str = colors.emerald,\n",
    "        secondary_hue: colors.Color | str = colors.emerald,\n",
    "        neutral_hue: colors.Color | str = colors.neutral,\n",
    "        spacing_size: sizes.Size | str = sizes.spacing_lg,\n",
    "        radius_size: sizes.Size | str = sizes.radius_lg,\n",
    "        text_size: sizes.Size | str = sizes.text_lg,\n",
    "        font: fonts.Font\n",
    "        | str\n",
    "        | Iterable[fonts.Font | str] = (\n",
    "            fonts.GoogleFont(\"Nanum Barun Gothic\"),\n",
    "            \"sans-serif\",\n",
    "        ),\n",
    "        font_mono: fonts.Font\n",
    "        | str\n",
    "        | Iterable[fonts.Font | str] = (\n",
    "            fonts.GoogleFont(\"Nanum Barun Gothic\"),\n",
    "            \"sans-serif\",\n",
    "        ),\n",
    "    ):\n",
    "        super().__init__(\n",
    "            primary_hue=primary_hue,\n",
    "            secondary_hue=secondary_hue,\n",
    "            neutral_hue=neutral_hue,\n",
    "            spacing_size=spacing_size,\n",
    "            radius_size=radius_size,\n",
    "            text_size=text_size,\n",
    "            font=font,\n",
    "            font_mono=font_mono,\n",
    "        )\n",
    "        super().set(\n",
    "            body_background_fill=\"#6da682\",\n",
    "            body_background_fill_dark=\"#6da682\",\n",
    "            \n",
    "            block_background_fill=\"#ffffff\",\n",
    "            block_background_fill_dark=\"#ffffff\",\n",
    "            block_shadow=\"0 4px 8px rgba(0, 0, 0, 0.1)\",\n",
    "            \n",
    "            button_primary_background_fill=\"linear-gradient(90deg, *primary_300, *secondary_400)\",\n",
    "            button_primary_background_fill_hover=\"linear-gradient(90deg, *primary_200, *secondary_300)\",\n",
    "            button_primary_text_color=\"black\",\n",
    "            button_primary_background_fill_dark=\"linear-gradient(90deg, *primary_600, *secondary_800)\",\n",
    "            \n",
    "            slider_color=\"*secondary_300\",\n",
    "            slider_color_dark=\"*secondary_600\",\n",
    "            block_title_text_weight=\"600\",\n",
    "            block_border_width=\"2px\",\n",
    "            button_primary_shadow=\"*shadow_drop_md\",\n",
    "            button_large_padding=\"24px\",\n",
    "            \n",
    "            input_background_fill=\"#ffffff\",\n",
    "            input_background_fill_dark=\"#ffffff\",\n",
    "            input_shadow=\"none\",\n",
    "        )\n",
    "\n",
    "football = Football()\n",
    "\n",
    "# CSSë¡œ Football ìŠ¤íƒ€ì¼ ì •ì˜\n",
    "css = \"\"\"\n",
    "/* ì „ì²´ ë°°ê²½ ì„¤ì • */\n",
    ".gradio-container {\n",
    "    background-color: #6da682;\n",
    "}\n",
    "\n",
    "/* ë¸”ë¡(ë°•ìŠ¤) ì„¤ì • ë° í…ìŠ¤íŠ¸ì— ê·¸ë¦¼ì ì¶”ê°€ */\n",
    ".gr-block {\n",
    "    background-color: #ffffff;\n",
    "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n",
    "    border-radius: 10px; /* ëª¨ì„œë¦¬ ë‘¥ê¸€ê¸° */\n",
    "    padding: 20px;\n",
    "    text-shadow: none;\n",
    "}\n",
    "\n",
    "/* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */\n",
    "button.primary {\n",
    "    background: linear-gradient(90deg, #66cdaa, #008b8b);\n",
    "    color: black;\n",
    "    padding: 12px 24px;\n",
    "    border-radius: 8px; /* ë²„íŠ¼ ëª¨ì„œë¦¬ ë‘¥ê¸€ê¸° */\n",
    "    box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.2);\n",
    "}\n",
    "\n",
    "button.primary:hover {\n",
    "    background: linear-gradient(90deg, #50a37c, #006b6b);\n",
    "}\n",
    "\n",
    "button.primary:active {\n",
    "    background: linear-gradient(90deg, #006b6b, #66cdaa);\n",
    "}\n",
    "\n",
    "/* ìŠ¬ë¼ì´ë” ìƒ‰ìƒ */\n",
    ".gr-slider .track-fill {\n",
    "    background-color: #66cdaa;\n",
    "}\n",
    "\n",
    "/* ì…ë ¥ í•„ë“œ ë°°ê²½ */\n",
    "input[type=\"text\"], textarea {\n",
    "    background-color: #ffffff;\n",
    "    border-radius: 8px; /* ì…ë ¥ í•„ë“œ ëª¨ì„œë¦¬ ë‘¥ê¸€ê¸° */\n",
    "    padding: 10px;\n",
    "    box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.1);\n",
    "}\n",
    "\n",
    "/* ë©”ì‹œì§€ ì°½ ì „ì²´ ë°°ê²½ ì„¤ì • */\n",
    ".gr-chatbot {\n",
    "    background-color: #ffffff; /* ë©”ì‹œì§€ ì°½ ì „ì²´ í°ìƒ‰ ë°°ê²½ */\n",
    "    border: none;\n",
    "    padding: 0;\n",
    "}\n",
    "\n",
    "/* ì „ì²´ ë°°ê²½ ì´ë¯¸ì§€ ì„¤ì • */\n",
    ".gradio-container {\n",
    "    background-image: url('https://static.vecteezy.com/system/resources/previews/013/950/541/non_2x/football-field-flat-flat-icon-free-vector.jpg');\n",
    "    background-size: cover;\n",
    "    background-position: center; /* ì´ë¯¸ì§€ ê°€ìš´ë° ì •ë ¬ */\n",
    "    background-repeat: no-repeat; /* ì´ë¯¸ì§€ ë°˜ë³µí•˜ì§€ ì•ŠìŒ */\n",
    "}\n",
    "\n",
    "/* ì œëª© í…ìŠ¤íŠ¸ì— ì¶”ê°€ì ì¸ ê·¸ë¦¼ì íš¨ê³¼ */\n",
    "#gradio-animation {\n",
    "    color: white;\n",
    "    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.6); /* ê·¸ë¦¼ì íš¨ê³¼ */\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# JavaScript ì½”ë“œ\n",
    "js = \"\"\"\n",
    "function createGradioAnimation() {\n",
    "    var container = document.createElement('div');\n",
    "    container.id = 'gradio-animation';\n",
    "    container.style.fontSize = '2em';\n",
    "    container.style.fontWeight = 'bold';\n",
    "    container.style.textAlign = 'center';\n",
    "    container.style.marginBottom = '20px';\n",
    "\n",
    "    var text = 'FootBotâš½ï¸';\n",
    "    for (var i = 0; i < text.length; i++) {\n",
    "        (function(i){\n",
    "            setTimeout(function(){\n",
    "                var letter = document.createElement('span');\n",
    "                letter.style.opacity = '0';\n",
    "                letter.style.transition = 'opacity 0.5s';\n",
    "                letter.innerText = text[i];\n",
    "\n",
    "                container.appendChild(letter);\n",
    "\n",
    "                setTimeout(function() {\n",
    "                    letter.style.opacity = '1';\n",
    "                }, 50);\n",
    "            }, i * 250);\n",
    "        })(i);\n",
    "    }\n",
    "\n",
    "    var gradioContainer = document.querySelector('.gradio-container');\n",
    "    gradioContainer.insertBefore(container, gradioContainer.firstChild);\n",
    "\n",
    "    return 'Animation created';\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import ChatUpstage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from collections import Counter\n",
    "import requests\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Function to fetch news articles from Naver Soccer News API\n",
    "def fetch_articles(api_url, headers):\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    articles = response.json()\n",
    "    return articles['items'][:100]\n",
    "\n",
    "# Function to clean and tokenize text\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.lower().split()\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate keyword frequency for importance\n",
    "def calculate_importance(article_text, common_keywords):\n",
    "    tokens = clean_and_tokenize(article_text)\n",
    "    keyword_count = sum(1 for token in tokens if token in common_keywords)\n",
    "    return keyword_count\n",
    "\n",
    "# Function to remove duplicate articles based on content similarity\n",
    "def remove_duplicates(articles, threshold=0.8):\n",
    "    descriptions = [article['description'] for article in articles]\n",
    "    vectorizer = TfidfVectorizer().fit_transform(descriptions)\n",
    "    similarity_matrix = cosine_similarity(vectorizer)\n",
    "    unique_articles = []\n",
    "    seen_indices = set()\n",
    "\n",
    "    for i, article in enumerate(articles):\n",
    "        if i not in seen_indices:\n",
    "            unique_articles.append(article)\n",
    "            similar_indices = np.where(similarity_matrix[i] > threshold)[0]\n",
    "            seen_indices.update(similar_indices)\n",
    "\n",
    "    return unique_articles\n",
    "\n",
    "# Fetch, process, and summarize articles\n",
    "def summarize_articles():\n",
    "    api_url = \"https://openapi.naver.com/v1/search/news.json?query=ì¶•êµ¬&display=100&sort=date\"\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": \"aMevyrYH7wOTUGOb7H7l\",\n",
    "        \"X-Naver-Client-Secret\": \"gSdqbjXchL\"\n",
    "    }\n",
    "\n",
    "    # Fetch and process articles\n",
    "    articles = fetch_articles(api_url, headers)\n",
    "    articles = remove_duplicates(articles)\n",
    "\n",
    "    # Aggregate text to find common keywords\n",
    "    all_text = \" \".join([article['description'] for article in articles if 'description' in article])\n",
    "    common_tokens = Counter(clean_and_tokenize(all_text)).most_common(20)\n",
    "    common_keywords = [token for token, _ in common_tokens]\n",
    "\n",
    "    # Initialize LLM and summarization chain\n",
    "    llm = ChatUpstage()\n",
    "    prompt_template = PromptTemplate(input_variables=[\"article\"], template=\"ë‹¤ìŒ ê¸°ì‚¬ë¥¼ ìš”ì•½í•´ ì£¼ì„¸ìš”: {article}\")\n",
    "    summary_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    # Summarize and rank articles by importance\n",
    "    important_articles = []\n",
    "    for article in articles:\n",
    "        if 'description' in article:\n",
    "            importance_score = calculate_importance(article['description'], common_keywords)\n",
    "            summary = summary_chain.run(article['description'])\n",
    "            title_prompt = f\"ì´ ê¸°ì‚¬ì— ì í•©í•œ ì œëª©ì„ ì§€ì–´ì£¼ì„¸ìš”: {article['description']}\"\n",
    "            title = llm.generate([title_prompt]).generations[0][0].text.strip()\n",
    "            important_articles.append((title, summary, importance_score))\n",
    "\n",
    "    # Sort and return top 3 articles\n",
    "    important_articles.sort(key=lambda x: x[2], reverse=True)\n",
    "    return important_articles[:3]\n",
    "\n",
    "# Gradio Interface\n",
    "def display_summaries():\n",
    "    articles = summarize_articles()\n",
    "    summary_md = \"\\n\\n\".join(\n",
    "        [f\"### ğŸ¤– **{title}**\\n{summary}\" for title, summary, _ in articles]\n",
    "    )\n",
    "    return summary_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜¤í”„ì‚¬ì´ë“œëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ë°œìƒí•˜ë‚˜ìš”?\n",
      "ì˜ë¡œìš°ì¹´ë“œì™€ ë ˆë“œì¹´ë“œì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\n",
      "ì¶•êµ¬ ì„ ìˆ˜ ìˆ˜ëŠ” ì´ ëª‡ëª…ì¸ê°€ìš”?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2018, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1565, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/utils.py\", line 813, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/gradio/chat_interface.py\", line 638, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/dw/4cmgh7qj7v9_ctks58g5x32c0000gn/T/ipykernel_32086/1042930721.py\", line 7, in chat\n",
      "    relevant_context = get_relevant_context(question, retriever)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/dw/4cmgh7qj7v9_ctks58g5x32c0000gn/T/ipykernel_32086/1646293657.py\", line 3, in get_relevant_context\n",
      "    relevant_docs = retriever.get_relevant_documents(question)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 182, in warning_emitting_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 397, in get_relevant_documents\n",
      "    return self.invoke(query, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 254, in invoke\n",
      "    raise e\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_core/retrievers.py\", line 247, in invoke\n",
      "    result = self._get_relevant_documents(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_core/vectorstores/base.py\", line 1080, in _get_relevant_documents\n",
      "    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py\", line 582, in similarity_search\n",
      "    docs_and_scores = self.similarity_search_with_score(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_chroma/vectorstores.py\", line 679, in similarity_search_with_score\n",
      "    query_embedding = self._embedding_function.embed_query(query)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/langchain_upstage/embeddings.py\", line 232, in embed_query\n",
      "    response = self.client.create(input=text, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/openai/resources/embeddings.py\", line 124, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/hanjiin/anaconda3/lib/python3.11/site-packages/openai/_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"'$.input' is invalid. Please check the API reference: https://developers.upstage.ai/docs/apis/embeddings\", 'type': 'invalid_request_error', 'param': 'input', 'code': None}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶•êµ¬ì—ì„œ ë¬´ìŠ¹ë¶€ëŠ” ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?\n",
      "ê³¨í‚¥ì€ ì–¸ì œ ì£¼ì–´ì§€ë‚˜ìš”?\n",
      "ê³¨í‚¥ì€ ì–¸ì œ ì£¼ì–´ì§€ë‚˜ìš”?\n",
      "ì˜¤í”„ì‚¬ì´ë“œëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ë°œìƒí•˜ë‚˜ìš”?\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=football, js=js) as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            gr.Markdown(\"<h1 style='text-align: center;'>ì˜¤ëŠ˜ì˜ ì¶•êµ¬ ë‰´ìŠ¤ ìš”ì•½</h1>\")\n",
    "            article_summaries = gr.Markdown(value=display_summaries, elem_id=\"article_summaries\")\n",
    "        with gr.Column():\n",
    "            chatbot = gr.ChatInterface(\n",
    "                chat,\n",
    "                examples=[\n",
    "                    \"ì˜¤í”„ì‚¬ì´ë“œëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ë°œìƒí•˜ë‚˜ìš”?\",\n",
    "                    \"íŒ¨ë„í‹°í‚¥ì€ ì–¸ì œ ì£¼ì–´ì§€ë‚˜ìš”?\",\n",
    "                    \"ì˜ë¡œìš°ì¹´ë“œì™€ ë ˆë“œì¹´ë“œì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "                ],\n",
    "                title=\"ê¶ê¸ˆí•œ ì‚¬í•­ì„ ê²€ìƒ‰í•´ë³´ì„¸ìš”\",\n",
    "                description=\"ëª¨ë¥´ëŠ” ìš©ì–´ë‚˜ ê·œì¹™ì„ ì§ˆë¬¸í•  ìˆ˜ ìˆì–´ìš”!\",\n",
    "            )\n",
    "            chatbot.chatbot.height = 300 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_kernal",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
